{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1. Set up Asyncio"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59652f003eaac394"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:10.267994Z",
     "start_time": "2024-12-10T18:50:10.260827Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Set up the Qdrant vector database"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "552553ecd4fce87a"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "collection_name=\"chat_with_docs\"\n",
    "\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:15.062284Z",
     "start_time": "2024-12-10T18:50:13.072118Z"
    }
   },
   "id": "163d94953086f4df"
  },
  {
   "cell_type": "markdown",
   "source": [
    "3. Read the documents"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cda58832ecaec9e"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:28.646941Z",
     "start_time": "2024-12-10T18:50:21.715114Z"
    }
   },
   "id": "74ffd81d232c346f"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(list, 32)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:33.463083Z",
     "start_time": "2024-12-10T18:50:33.452324Z"
    }
   },
   "id": "721053ccfb9ce916"
  },
  {
   "cell_type": "markdown",
   "source": [
    " 4. A function to index data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1ad8d5bca2e1ac12"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "    return index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:40.167554Z",
     "start_time": "2024-12-10T18:50:40.151180Z"
    }
   },
   "id": "c57ecd4891a8db71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "5. Load the embedding model and index data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1014b18e8acf257f"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()  # If using CUDA\n",
    "torch.mps.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:42:47.935462Z",
     "start_time": "2024-12-10T18:42:47.930967Z"
    }
   },
   "id": "8f090ce2e6cf7d67"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:53.093822Z",
     "start_time": "2024-12-10T18:50:53.081197Z"
    }
   },
   "id": "4ecdee1aa11ab8ed"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "tensor = torch.tensor([1.0, 2.0, 3.0, 6.0])\n",
    "array = tensor.numpy()\n",
    "print(array)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:50:59.203667Z",
     "start_time": "2024-12-10T18:50:56.018365Z"
    }
   },
   "id": "3741e07cae179a8c"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:51:28.801892Z",
     "start_time": "2024-12-10T18:51:00.302526Z"
    }
   },
   "id": "a238d413171e1619"
  },
  {
   "cell_type": "markdown",
   "source": [
    "6. Load the LLM"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84e8684e29bb1c91"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:51:28.984768Z",
     "start_time": "2024-12-10T18:51:28.804344Z"
    }
   },
   "id": "4dafce45fb6949ce"
  },
  {
   "cell_type": "markdown",
   "source": [
    "7. Define the prompt template"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "71eac6537d5302ab"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:52:07.700267Z",
     "start_time": "2024-12-10T18:52:07.680387Z"
    }
   },
   "id": "675fbdc2b5a47cf1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "8.Reranking"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "820cb25f96442223"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:52:35.820428Z",
     "start_time": "2024-12-10T18:52:34.984820Z"
    }
   },
   "id": "4820eeb3d53bb4ef"
  },
  {
   "cell_type": "markdown",
   "source": [
    "9. Query the document"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1decdc2b707efc13"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:55:17.137780Z",
     "start_time": "2024-12-10T18:55:10.487617Z"
    }
   },
   "id": "3de47b3306770a07"
  },
  {
   "cell_type": "markdown",
   "source": [
    "10. Display the responce"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74acafeccb7f73c6"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.Markdown object>",
      "text/markdown": "DSPy stands for Deep Speech Processing, which is an open-source Python library used for natural language processing (NLP) tasks. It provides a framework for implementing various NLP techniques, including text analysis, sentiment analysis, language modeling, and more. The library allows users to define their own custom models and interfaces using a variety of signatures, which are essentially typed declarations of functions that take input fields and output fields as arguments. This enables the creation of customized prompts, models, and interfaces for specific tasks and applications."
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-10T18:55:20.155101Z",
     "start_time": "2024-12-10T18:55:20.143975Z"
    }
   },
   "id": "a00a5ab5e905de87"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "24196672d7453aa0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
